---
title: "Results_paper1"
author: "Amieroh Abrahams"
date: "07 April 2019"
output: html_document
---

This scripts comprise of the analyses needed for the first section of my masters project. 

# Loading libraries
First I need to find, install and load various packages. These packages will be available on CRAN and can be accessed and installed in the usual way.
 
```{r prelim_opts, echo=FALSE}
knitr::opts_chunk$set(
  comment = "R>",
  warning = FALSE,
  message = FALSE 
)

library(tidyverse)
library(plyr)
library(lubridate)
library(ggpubr)
library(zoo)
library(lubridate)
library(FNN)
library(forecast)
library(astrochron)
library(WaveletComp)
library(data.table)
library(heatwaveR)
library(viridis)
library(ggrepel)
library(plyr)
library(maptools)
library(sp)
library(geosphere)
library(marmap)
library(PBSmapping)
library(scales)
library(grid)
library(gridExtra)
library(circular)
library(fossil)
library(mapproj)
library(rgeos)
library(ncdf4)
# library(sinkr)
library(spacetime)
library(reshape2)
library(plyr) # Never load plyr when also loading the tidyverse. It causes a lot of conflicts.
library(stringr)
library(doMC); doMC::registerDoMC(cores = 4)
library(fasttime)

# library(devtools)
# install_github("marchtaylor/sinkr")

## Functions
source("functions/earthdist.R")
source("functions/wind.rose.R")
```

## Load site list SACTN data

Now to get to the data. The first step involves the loading of the site list. The statistical properties of the seawater temperature representing the South African coastline, such as the mean, minimum and maximum temperatures. These values vary among coastal sections due to the influence of the cold Benguala and warm Agulhas currents. Here we will only focus on the temperature data found along the west coast (wc) (i.e. sites influenced by the Benguela current, EBUS). The SACTN dataset comprise of 129 *in situ* coastal seawater temperatures derived from daily measurements over up to 40 years. The SACTN temperature dataset was compiled by measuring coastal temperatures at 129 sites along the coast of South Africa, daily from 1972 until 2017. 

```{r load_files1, include=TRUE}
load("Data_P1/site_list_v4.2.RData")
load("Data_P1/SACTN_daily_v4.2.RData")
```

## Selecting the sites and creating new *in situ* datasets

Now we select only the sites occuring along the west coast. The purpose of this is to assess whether or not the intensity of upwelling varies at a range of distances from the coastline. The data used here is obtained from different satellites and thus represent different resolutions. I then find an overlapping time series for each of the sites along the west coast. The length of time series for each of the sites is greater than 10 years. The sites along the west coast vary between upwelling centres and further away from the upwelling centres. As a result it is hypothesised that sites located within the upwelling centre may resemble a longer more intense upwelling event when compared to sites located further away from the upwelling centre. For this analyses a long term time series of 30years is not required and so our time series starts in 1992 - 2015.

```{r}
site_list_sub <- site_list %>%
  filter(coast == "wc") %>%
  filter(length > 3650) 
site_list_sub <- site_list_sub[c(-2, -3, -5, -6, -7,-9, -12, -13, -14),] # Here I keep all the sites with temperature obtained up to 2017
# save(site_list_sub, file = "Data_P1/site_list_sub.Rdata")

SACTN_US <- SACTN_daily_v4.2 %>%
  left_join(site_list[,c(4,13)], by = "index") %>%
  filter(index %in% site_list_sub$index) %>%
  separate(index, into = c("site", "src"), sep = "/", remove = FALSE) %>%
  dplyr::rename(insitu_temp = temp)

# The next step is to identify the period where all sites within this dataset overlap
SACTN_overlap <- SACTN_US %>% 
  filter(year(date) %in% seq(1992, 2015)) %>% # The length of the time may vary depending on the question
  drop_na() # Removing NA values within the dataset
# save(SACTN_overlap, file = "Data_P1/SACTN_overlap.RData")

# Visualising the data
temp_plot <- function(df){
  plot <- ggplot(data = df, aes(x = date, y = insitu_temp, colour = site)) +
    geom_line(aes(group = site)) +
    labs(x = "", y = "Temperature (°C)") +
    theme(axis.text.x = element_text(angle = 45)) +
    theme(legend.position = "top")
}

SACTN_plot <- temp_plot(df = SACTN_overlap)
```

# Remotely sensed SST datasets
Matching the satellite sea surface temperature data point to the in situ temperature collected data points.

## MUR dataset

```{r}
MUR_Lamberts_Bay <- read_csv("Data/MUR_nearest5pixels/MUR_Lamberts Bay_SST_timeseries_5nearest.csv")
MUR_Port_Nolloth <- read_csv("Data/MUR_nearest5pixels/MUR_Port Nolloth_SST_timeseries_5nearest.csv")
MUR_Saldanha_Bay <- read_csv("Data/MUR_nearest5pixels/MUR_Saldanha Bay_SST_timeseries_5nearest.csv")
MUR_Yzerfontein <- read_csv("Data/MUR_nearest5pixels/MUR_Yzerfontein_SST_timeseries_5nearest.csv")
MUR_Kommetjie <- read_csv("Data/MUR_nearest5pixels/MUR_Kommetjie_SST_timeseries_5nearest.csv")
MUR_Sea_Point <- read_csv("Data/MUR_nearest5pixels/MUR_Sea Point_SST_timeseries_5nearest.csv")

MUR_SST <- rbind(MUR_Lamberts_Bay,MUR_Yzerfontein, MUR_Port_Nolloth,MUR_Saldanha_Bay, MUR_Kommetjie, MUR_Sea_Point) %>%
  dplyr::rename(site = station)

MUR_SST$date <- (ymd(MUR_SST$date))
MUR_SST <- MUR_SST %>% 
  dplyr::rename(temp = nearest1) %>% 
  drop_na()

# save(MUR_SST, file = "Data_P1/MUR_SST.RData")
```

This function matches the remotely-sensed SST to the *in-situ* collected SST

```{r}
match_func <- function(df){
  match <- SACTN_overlap  %>%  
  left_join(df, by = c("site", "date")) %>% 
  na.trim()
  return(match)
}

# Basic visualisations
match_plot <- function(df){
  plot1 <- df %>% 
ggplot(aes(x = date, y = temp)) +
  geom_hline(aes(yintercept = mean(temp)), colour = "salmon") +
  geom_line() +
  facet_wrap(~ site, nrow = 2) +
  theme_bw()
  return(plot1)
}

temp_plot <- function(df){
  temp_plot <- df %>% 
  ggplot(aes(x = date, y = temp, colour = site)) +
    geom_line(aes(group = site)) +
    labs(x = "", y = "Temperature (°C)") +
    theme(axis.text.x = element_text(angle = 45)) +
    theme(legend.position = "top")
}

## Matching the in-situ data with the G1SST SST data
insitu_MUR <- match_func(df = MUR_SST) %>%
  drop_na()
# save(insitu_MUR, file = "Data_P1/insitu_MUR.RData")

MUR_plot <- match_plot(df = insitu_MUR)
MUR_plot <- temp_plot(df = insitu_MUR)
```

## G1SST dataset

```{r}
Lamberts_Bay <- read_csv("Data/G1SST_sub/Lamberts Bay_SST_timeseries_5nearest.csv")
Port_Nolloth <- read_csv("Data/G1SST_sub/Port Nolloth_SST_timeseries_5nearest.csv")
Saldanha_Bay <- read_csv("Data/G1SST_sub/Saldanha Bay_SST_timeseries_5nearest.csv")
Yzerfontein <- read_csv("Data/G1SST_sub/Yzerfontein_SST_timeseries_5nearest.csv")
Kommetjie_SST <- read_csv("Data/G1SST_sub/Kommetjie_SST_timeseries_5nearest.csv")
Sea_Point_SST<- read_csv("Data/G1SST_sub/Sea Point_SST_timeseries_5nearest.csv")

G1SSTsub_SST <- rbind(Lamberts_Bay, Port_Nolloth, Saldanha_Bay, Yzerfontein, Kommetjie_SST, Sea_Point_SST) %>%
  dplyr::rename(site = station)

G1SSTsub_SST$date <- (ymd(G1SSTsub_SST$date))
# save(G1SSTsub_SST, file = "Data_P1/G1SSTsub_SST.RData")

G1SSTsub_SST <- G1SSTsub_SST %>% 
  drop_na() %>% 
  dplyr::rename(temp = nearest1)

## Matching the Insitu data with the G1SST SST data
insitu_G1SST <- match_func(df = G1SSTsub_SST) 
# save(insitu_G1SST, file = "Data_P1/insitu_G1SST.RData")

G1SST_plot <- match_plot(df = insitu_G1SST )
G1SST_temp_plot <- temp_plot(df = insitu_G1SST)
```

## K10 dataset

```{r}
Lamberts_Bay <- read_csv("Data/K10/K10_Lamberts Bay_SST_timeseries_5nearest.csv")
Port_Nolloth <- read_csv("Data/K10/K10_Port Nolloth_SST_timeseries_5nearest.csv")
Saldanha_Bay <- read_csv("Data/K10/K10_Saldanha Bay_SST_timeseries_5nearest.csv")
Yzerfontein <- read_csv("Data/K10/K10_Yzerfontein_SST_timeseries_5nearest.csv")
Kommetjie <- read_csv("Data/K10/K10_Kommetjie_SST_timeseries_5nearest.csv")
Sea_Point <- read_csv("Data/K10/K10_Sea Point_SST_timeseries_5nearest.csv")

K10_SST <- rbind(Lamberts_Bay, Yzerfontein, Port_Nolloth, Saldanha_Bay, Kommetjie, Sea_Point) %>%
  dplyr::rename(site = station)
K10_SST$date <- (ymd(K10_SST$date))
save(K10_SST, file = "Data_P1/K10_SST.RData")

K10_SST <- K10_SST %>% 
  drop_na()
K10_SST <- K10_SST %>% 
  dplyr::rename(temp = nearest1)

## Matching the Insitu data with the K10 SST data
insitu_K10 <- match_func(df = K10_SST) 
# save(insitu_K10, file = "Data_P1/insitu_K10.RData")

K10_plot <- match_plot(df = insitu_K10)
k10_temp_plot <- temp_plot(df = insitu_K10)
```

## OISST dataset

```{r}

OISSTDir <- "~/Documents/Masters_2019/Data_SST"
OISST <- fread(paste0(OISSTDir, "/csvavhrr-only-v2-19810901-20180630.csv"),
            col.names = c("lon", "lat", "temp", "date"))

# Visualise the data
# To explore the data I visualise the min temperatures along the South african coastline.

OISST %>%
  filter(date == min(date)) %>%
  ggplot(aes(x = lon, y = lat)) +
  geom_raster(aes(fill = temp))
```

## The nearest SST pixels
Now we apply the FNN (Fast Nearest Neighbor) package to determine the nearesr SST pixel to the insitu collected sites. 

```{r}

load("Data_P1/site_list_sub.Rdata") # Loading the SACTN_overlap dataset which allows me to match it to the OISST data 

unique_pixel <- OISST %>%
  select(lon, lat) %>%
  unique()

# Select nearest 1 pixels (k = 1)
# Here I use knnx to find the closes 1 pixels to the insitu sites
match_index <- knnx.index(data = as.matrix(unique_pixel[,1:2]),
                      query = as.matrix(site_list_sub[,5:6]), k = 1)

pixel_match <- unique_pixel[match_index,] %>%
  unite(col = combi, lon, lat, sep = "/", remove = F) %>%
  mutate(site = site_list_sub$site)

# Subsetting the OISST data to match the upwelling sites within the in situ collected temperature data
OISST_match <- OISST %>%
  unite(col = combi, lon, lat, sep = "/", remove = F) %>%
  filter(combi %in% pixel_match$combi)
# save(OISST_match, file = "Data_P1/OISST_match.RData")

OISST_sites <- OISST_match %>%
  left_join(pixel_match, by = c("combi", "lon", "lat")) %>%
  dplyr::rename(temp_OISST =temp)

OISST_sites <- OISST_sites %>% 
  dplyr::rename(temp = temp_OISST) %>%
  dplyr::mutate(date = as.Date(date)) %>% 
  drop_na()
# save(OISST_sites, file = "Data_P1/OISST_sites.RData")

## Matching the Insitu data with the OISST SST data
insitu_OISST <- match_func(df = OISST_sites)
# save(insitu_OISST, file = "Data_P1/insitu_OISST.RData")

OISST_plot <- match_plot(df = insitu_OISST)
OISST_temp_plot <- temp_plot(df = insitu_OISST)
```

Loading in the matched satellite temperature data

```{r}
load("Data_P1/SACTN_overlap.RData")
load("Data_P1/insitu_MUR.RData")
load("Data_P1/insitu_OISST.RData")
load("Data_P1/insitu_G1SST.RData")
load("Data_P1/insitu_K10.RData")
```

## Wavelets
Wavelet anlyses showing the temperature variation between sites along the west coast for the years 2010 - 2014. Working with remotely-sensed SST and *in-situ* collected coastal seawater temperature, in the analyses bellow I aim to show how the intensity and duration of upwelling events vary between sites along the west coast of South Africa. It is important to remember that each of the datasets used within this study obtained SSTs at different resolutions. 

## Load in the datasets

```{r}
load("Data_P1/SACTN_overlap.RData")
load("Data_P1/insitu_MUR.RData")
load("Data_P1/insitu_OISST.RData")
load("Data_P1/insitu_G1SST.RData")
load("Data_P1/insitu_K10.RData")
```

# Time series

Creating a time series from the year 2010 to 2014 for both the satellite and in situ collected SST data. This is done to provide an accurate comparison between datasets given that the different datasets have different starting and ending dates for which temperature was collected. E.g. Temperature for the MUR  only started in 2002

```{r}
filtered_years <- function(df){
  upwelling<- df %>% 
  filter(year(date) %in% seq(2010, 2014)) %>% 
  drop_na()
}

SACTN_fyears <- filtered_years(df = SACTN_overlap) 
MUR_fyears <- filtered_years(df = insitu_MUR) 
OISST_fyears <- filtered_years(df = insitu_OISST) 
G1SST_fyears <- filtered_years(df = insitu_G1SST) 
K10_fyears <- filtered_years(df = insitu_K10) 
```

# Wavelet analyses

```{r}
# When doing a wavelet analyses on the SACTN dataset the "insitu_temp" column needed to be renamed to "temp" for the rest of the code to run. The rest of this code is kept constant for each of the datasets (Remotely-sensed SST and In-situ seawater temperature)

SACTN_fyears <- SACTN_fyears %>%
  dplyr::rename(temp = insitu_temp)

temp.d <- function(df){
temp.d <- df %>% 
  dplyr::group_by(site, date) %>% 
  dplyr::summarise(temp = mean(temp, na.rm = T)) %>%
  dplyr::mutate(no = seq(1:n())) %>%
  dplyr::ungroup() %>% 
  dplyr::select(site, no, temp, date)
}

temp.d <- temp.d(df = SACTN_fyears) 
# temp.d <- temp.d(df = MUR_fyears) 
# temp.d <- temp.d(df = OISST_fyears)
# temp.d <- temp.d(df = G1SST_fyears) 
# temp.d <- temp.d(df = K10_fyears) 

temp.d <- temp.d %>% 
  select(site, no, temp, date)

prewhite_fun <- function(x) {
  df <- x[, 2:3]
  out <- prewhiteAR(df, order = 3, method = "mle", aic = TRUE,
             genplot = FALSE, verbose = FALSE)
  colnames(out) <- c("no", "temp")
  return(out)
  }
PN_prewhite <- as.tibble(ddply(temp.d, .(site), prewhite_fun))

ggplot(PN_prewhite, aes(x = no, y = temp)) +
  geom_hline(aes(yintercept = mean(temp)), colour = "salmon") +
  geom_line() +
  facet_wrap(~ site, nrow = 4) +
  theme_bw()

wl.fun <- function(x) {
  analyze.wavelet(x, "temp", loess.span = 0, dt = 1,
                   dj = 1/50, lowerPeriod = 2, make.pval = TRUE, n.sim = 50, 
                   method = "white.noise", verbose = FALSE)
}

PN_wave <- dlply(PN_prewhite, .(site), wl.fun, .parallel = T) # RWS: Allow parallel processing

for (i in 1:length(PN_wave)) {
  attributes(PN_wave[[i]]) <- c(attributes(PN_wave[[i]]), ref = names(PN_wave)[i])
}
# x <- PN_wave[[3]]
plot_fun <- function(x, plot_name = attributes(x)$ref) {
  png(filename = paste0("Figures",plot_name,"_wavelets.png"),
      width = 800, height = 600, units = "px", pointsize = 12, bg = "white")
  wt.image(x, siglvl = 0.05, col.contour = "black", color.key = "quantile",
           timelab = "Days", verbose = FALSE, useRaster = TRUE,
           periodlab = "Period", lwd = 1, graphics.reset = FALSE,
           main = plot_name)
  dev.off()
}
ldply(PN_wave, plot_fun, .parallel = T) # RWS: Allow parallel processing
```

#########################################################################################################################

# Calculating a shore transect

The aim of this is to find the temperature collected at different distances from the coast (distances = 15km, 25km, 35km and 50km). Then create wavelet analyses (or another analyses) where I compare how the intensity and duration of upwelling vary at different distances from the coastline. So would I have a dataset with temperature at 15km then at 25km then at 50km and so on...?

```{r}
load("Data_P1/site_list_sub.Rdata")
west <- site_list_sub
west$coast <- "west" # Chnages wc to west

load("Data_P1/africa_coast.RData")

## Downloading the bathy data from NOAA
# Download mid-res bathymetry data
# sa_lat <- c(-38, -24.5); sa_lon <- c(11.5, 35.5)
# sa_bathy <- as.xyz(getNOAA.bathy(lon1 = sa_lon[1], lon2 = sa_lon[2], lat1 = sa_lat[1], lat2 = sa_lat[2], resolution = 4))
# colnames(sa_bathy) <- c("lon", "lat", "depth")
# sa_bathy <- sa_bathy[sa_bathy$depth <= 0,]
# save(sa_bathy, file = "Data_P1/bathy/sa_bathy.RData")

# Loading in the newly downloaded bathymetry data
load("Data_P1/bathy/sa_bathy.RData")

# This function takes one site (e.g. one set of lon/lats) and calculates a shore normal transect
shore.normal.transect <- function(site, width = 2){
  # Find the site on the coastline and it's nearest neighbour points
  coords <- data.frame(lon = site$lon, lat = site$lat)
  coords2 <- knnx.index(africa_coast[,1:2], as.matrix(coords), k = 1)
  coords3 <- data.frame(site = site$site, africa_coast[c(coords2-width, coords2+width),]) 
  coords3 <- coords3[2:1,1:3]
  # Define the shore normal transect bearing
  heading <- earth.bear(coords3[1,2], coords3[1,3], coords3[2,2], coords3[2,3]) + 90
  if(heading >= 360){
    heading <- heading-360
  } else {
    heading <- heading
  }
  heading2 <- data.frame(site = site$site, lon = site$lon, lat = site$lat, heading)
  return(heading2)
}

# Creating the transects
site_transects <- data.frame()
for(i in 1:length(west$site)){
 site <- west[i,]
 site_transect <- shore.normal.transect(site, 2)
 site_transects <- rbind(site_transects, site_transect)
}

# Manually correcting Sea Point and Kommetjie
site_transects$heading[5:6] <- 290 
save(site_transects, file = "Data_P1/site_transects.RData")
load("Data_P1/site_transects.RData")

# This function takes one site (e.g. one set of lon/lats) and calculates a shore normal transect
# It then extracts a lat/ lon point every X kilometres until reaching a specified isobath

transect.pixel <- function(site, distances){
  # Extract coordinates
  coords <- data.frame(lon = site$lon, lat = site$lat)
  # Find lon/ lats every X metres 
  pixels <- data.frame()
  # deep <- 999
  # distance_multiplier <- 1
  # while(deep > isobath){
  for(i in 1:length(distances)){
    # Add index to show distance from coast
    # Find depth at distance step
    coords2 <- as.data.frame(destPoint(p = coords, b = site$heading, d = distances[i]))
    sitesIdx <- knnx.index(sa_bathy[,1:2], as.matrix(coords2), k = 1)
    bathy2 <- sa_bathy[sitesIdx,]
    bathy2 <- bathy2[complete.cases(bathy2$depth),]
    bathy3 <- data.frame(site = site$site, lon = bathy2$lon, lat = bathy2$lat, 
                         heading = site$heading, 
                         # depth = bathy2$depth,
                         distance = distances[i])
    pixels <- rbind(pixels, bathy3)
    coords <- coords2
    # distance_multiplier <- distance_multiplier +1 
    # deep <- bathy2$depth
  }
  # pixels2 <- pixels[pixels$depth > -999,] 
  if(nrow(pixels) < 1){
    pixels <- data.frame(site, depth = NA)
  }else{
    pixels <- pixels
  }
  return(pixels)
}

# Pixel points
site_pixels <- data.frame()
for(i in 1:length(west$site)){
  site <- site_transects[i,]
  site_pixel <- transect.pixel(site, c(15000, 25000, 35000, 45000, 55000)) # RWS: The desired distances
  site_pixels <- rbind(site_pixels, site_pixel)
}

# save(site_pixels, file = "Data_P1/site_pixels.RData")
load("Data_P1/site_pixels.RData")

# Bounding box
  # Only one is made in order to know how large the the geom_point() squares should be made to match
test <- data.frame(xmin = destPoint(p = site_pixels[1,2:3], b = 270, d = 12500)[1],
                   xmax = destPoint(p = site_pixels[1,2:3], b = 90, d = 12500)[1],
                   ymin = destPoint(p = site_pixels[1,2:3], b = 180, d = 12500)[2],
                   ymax = destPoint(p = site_pixels[1,2:3], b = 0, d = 12500)[2])
```

## Create a visualisation of the sites at the different distances from the coastline
****Fix: Fill to show the temperature variation at Benguela coast| also fix the site names*

```{r}

load("Data_P1/south_africa_coast.RData")
load("Data_P1/bathy/sa_bathy.RData")
names(south_africa_coast)[1] <- "lon"

# Manually divide up coastline
wc <- south_africa_coast[291:410,]

# Define plotting parameters
sa_lats <- c(-37, -27); sa_lons <- c(14, 34)

# Plotting with pixel boxes
plot1 <- ggplot() +
 geom_contour(data = bathy[bathy$depth >= -250,], aes(x = lon, y = lat, z = depth),
               colour = "grey20", alpha = 0.7, size = 0.2, binwidth = 200, na.rm = TRUE, show.legend = FALSE) +
  stat_contour(data = sa_bathy[sa_bathy$depth < -250,], aes(x = lon, y = lat, z = depth, alpha = ..level..),
               colour = "grey20", size = 0.1, binwidth = 1000, na.rm = TRUE, show.legend = FALSE) +
  geom_polygon(data = south_africa_coast, aes(x = lon, y = lat, group = group), 
               size = 0.5, colour = "black", fill = "white") +
   # geom_text(data = west, aes(x = lon, y = lat, label = site),
   #          size = 1.8, colour = "black") +
  # geom_polygon(data = wcBox, aes(x = lon, y = lat, group = group), alpha = 0.20, 
  #              colour = "bisque1", fill = "bisque1") +
  geom_point(data = west, aes(x = lon, y = lat), alpha = 0.8, size = 1.6) +
  geom_text(data = west, aes(x = lon, y = lat, label = site), 
                     size = 6, box.padding = 2, nudge_x = -0.5, nudge_y = 0.2, 
                     segment.alpha = 0.4, force = 0.1) +
  geom_point(data = site_pixels, aes(x = lon, y = lat), colour = "black", shape = 0, alpha = 0.8, size = 2.1) +
  scale_alpha_continuous(breaks = c(-200, -1000, -2000, -3000, -4000, -5000),
                         guide_legend(title = "depth (m)")) +
  scale_fill_gradient(low = "dodgerblue4", high = "dodgerblue", na.value = "steelblue4", 
                      breaks = c(-1000, -2000, -3000, -4000, -5000),
                      guide_legend(title = "depth (m)")) +
   annotate("text", label = "INDIAN\nOCEAN", x = 34.00, y = -35.0,
             size = 6.0, angle = 0, colour = "black") +
    annotate("text", label = "ATLANTIC\nOCEAN", x = 14.00, y = -35.0,
             size = 6.0, angle = 0, colour = "black") +
    annotate("text", label = "Benguela", x = 15.0, y = -31.7,
             size = 4.0, angle = 302, colour = "black") +
    annotate("text", label = "Agulhas", x = 32.0, y = -31.7,
             size = 4.0, angle = 50, colour = "black") +
  labs(title = NULL, x = NULL, y = NULL) +
  theme(legend.key = element_rect(colour = NA, size = 0.2),
        legend.key.height = unit(0.4, "cm"),
        legend.background = element_blank()) +
  coord_cartesian(xlim = sa_lons, ylim = sa_lats) 
plot1
```

# Wind data

Loading the Wind data. The wind dataset was supplied by the SAWS in .text format. Wind data was provided for the sites closes to the requested site as wind data may not be available for the 6 study sites in this research.  The wind direction was collected every 2 hours and by using the circular function we calcuated the daily wind direction and mean speed. There is a numerically large gap between 360 and 2 where as in degrees its not as large. The circular mean function returns the mean direction of a vector of circular data. https://cran.r-project.org/web/packages/circular/circular.pdf (Pg 130). The circular function creates circular objects around the wind direction. 
  
```{r}
wind_1 <- read.delim("Data_P1/Wind_data/wind_data.txt(SAWS)/wind1/wind1.txt", na.strings = "", 
                     col.names = c("station_number", "station_name", "date", "hour", "sub", "speed", "dir"))

wind_2 <- read.delim("Data_P1/Wind_data/wind_data.txt(SAWS)/wind2/wind2.txt", na.strings = "",
                     col.names = c("station_number", "station_name", "date", "hour", "sub" ,"speed", "dir"))

wind_3 <- read.delim("Data_P1/Wind_data/wind_data.txt(SAWS)/wind3/wind3.txt", na.strings = "",
                     col.names = c("station_number", "station_name", "date", "hour", "sub" ,"speed", "dir"))

# Slecting the important columns for each of the datasets
wind_fix <- function(df){
wind <- df %>% 
  select(station_name, date, hour, dir, speed) %>%  #column names
  mutate(date = as.Date(as.character(date)),
         hour = as.numeric(as.character(hour)), 
         dir = as.numeric(as.character(dir)),
         speed = as.numeric(as.character(speed)))
}
# RWS: We can see when we force the values to be numeric that there are some non-numeric values in the base data
wind_1 <- wind_fix(df = wind_1)
wind_2 <- wind_fix(df = wind_2)
wind_3 <- wind_fix(df = wind_3)

## Renaming the sites within the wind datasets to match the name of the sites at which seawater temperature was collected
## The wind data was obtained from the SAWS and the wind stations used were the closes stations to which temperature was collected

renaming_sites_1 <- function(df) {
  sites <- df %>%
      # RWS: An alternative way to replace values without having to use multiple ifelse() statements
      mutate(temp_sites = case_when(station_name == "CAPE COLUMBINE" ~ "St Helena Bay",
                                    station_name  == "KOINGNAAS" ~ "Hondeklipbaai",
                                    station_name  == "PORT NOLLOTH" ~"Port Nolloth",
                                    station_name  == "LAMBERTSBAAI NORTIER" ~ "Lamberts Bay",
                                    station_name  == "LANGEBAANWEG AWS" ~ "Saldanha Bay"))
  return(sites)
}



wind_sitesmatched_1 <-  renaming_sites_1(df = wind_1)
# RWS: You should never be removing values by specific call like this
  # Rather you should be able to use some sort of conditional to screen out unwanted values
wind_sitesmatched_1 <- wind_sitesmatched_1[-c(121352, 121353, 379892, 379893, 609324, 609325, 843506, 843507, 1014585), ]

renaming_sites_2 <- function(df) {
  sites <- df %>%
    mutate(temp_sites = ifelse(station_name %in% c("ROBBENEILAND"), "Koeberg Basin",        
                           ifelse(station_name %in% c("GEELBEK"), "Yzerfontein",
                                ifelse(station_name %in% c("DASSEN ISLAND"), "Dassen Island",
                                       ifelse(station_name %in% c("ATLANTIS"), "Koeberg Basin","Error")))))
  return(sites)
}

wind_sitesmatched_2 <-  renaming_sites_2(df = wind_2)
wind_sitesmatched_2 <- wind_sitesmatched_2[-c(228372, 228373, 313441, 313442, 364881, 364882, 557392), ] 

renaming_sites_3 <- function(df) {
  sites <- df %>%
    mutate(temp_sites = ifelse(station_name %in% c("CAPE TOWN SLANGKOP"), "Kommetjie",  
                           ifelse(station_name %in% c("CAPE TOWN - ROYAL YACHT CLUB"), "Sea Point",
                           ifelse(station_name %in% c("CAPE TOWN TABLE BAY"), "Sea Point","Error"))))
  return(sites)
}

wind_sitesmatched_3 <-  renaming_sites_3(df = wind_3)
wind_sitesmatched_3 <- wind_sitesmatched_3[-c(119317, 119318, 196551, 196552, 346759), ] 

## CAPE TOWN SLANGKOP may be used for Kommetjie and for Houtbay
wind_3_HoutBay <- wind_3 %>% 
  filter(station_name == "CAPE TOWN SLANGKOP") %>% 
  mutate(temp_sites = ifelse(station_name %in% c("CAPE TOWN SLANGKOP"), "Hout Bay","Error"))

wind_data <- rbind(wind_3_HoutBay,wind_sitesmatched_3,wind_sitesmatched_2,wind_sitesmatched_1)
wind_data <- wind_data %>% 
  na.omit()

wind_data <- wind_data %>% 
  # mutate(date = as.Date(date)) %>%  
  group_by(date, hour, temp_sites) 

# write.csv(wind_data, file = "Data_P1/wind_data.csv", row.names = T)
# save(wind_data, file = "Data_P1/wind_data.RData")

# Loading the wind data
library(tidyverse)
library(circular)

# wind_data <- read_csv("wind_data.csv", col_types = cols(X1 = col_number(), date = col_character(), dir = col_number(), hour = col_number(),  speed = col_number())) # Dataset too large for Github but better data at line 638

# wind_data <- read_csv("Data_P1/wind_data.csv")

wind_daily <- wind_data %>%
  # ungroup() %>%
  # mutate(date = as.Date(date))%>%
  dplyr::group_by(temp_sites, date) %>%
  dplyr::summarise(dir_circ = round(mean.circular(circular(dir, units = "degrees")),2),
                   mean_speed = round(mean(speed),2))

# save(wind_daily, file = "Data_P1/wind_daily.RData")
# Loading in the daily wind data
load("Data_P1/wind_daily.RData")

# Subsetting to particular sites 
selected_sites <- c("Port Nolloth", "Lamberts Bay", "Sea Point", "Saldanha Bay", "Yzerfontein", "Kommetjie") 

wind_daily_sub <- wind_daily %>% 
  dplyr::rename(site = temp_sites) %>% 
  filter(site %in% selected_sites)

# save(wind_daily_sub, file = "Data_P1/wind_daily_sub.RData")
# Loading in the daily wind data
# load("Data_P1/wind_daily_sub.RData")
```
 
Matching wind data to the in situ collected data. This is done in order to plot the wind patterns for the same date at which temperature was collected. The satellite obtained SST data is also matched by date to the in situ temperature data and so the dates correspond. 

```{r}
insitu_wind <- match_func(df = wind_daily_sub) %>%
  drop_na()
insitu_wind <- insitu_wind %>% 
  filter(year(date) %in% seq(2010, 2014)) # The years 2010-2014 is the years of interest and the years for which wavelets are/will be created.
  
# save(insitu_wind, file = "Data_P1/insitu_wind.RData")
# Loading in the daily wind data
# load("Data_P1/insitu_wind.RData")
```

# Plotting the wind data - Waverose diagrams

```{r}
wave_daily_renamed <- insitu_wind %>% 
  dplyr::rename(spd = mean_speed) %>%
  dplyr::rename(dir = dir_circ) 

p.wr2 <- plot.windrose(data = wave_daily_renamed,
              spd = "spd",
              dir = "dir")
p.wr3 <- p.wr2 + facet_wrap(.~ site, ncol = 2, nrow = "") +
  theme(strip.text.x = element_text(size = 25))
p.wr3 # RWS: You'll want to remove the NA part in the figure output
```

## Chlorophyll-a data

Extract the chlorophyll a netCDF files and convert it to CSV. The chlorophylla data were obtained from MODIS Aqua. The aim of working with these chlorophyll a data is to examine how the chlorophyll a concentration varies with upwelling. This function convert chlorophyll netCDF datasets to a single CSV dataset.

```{r}
# MODIS_chlor.dir <- "/home/amieroh/Documents/Data/Datasets/Chlorophyll_a"
# MODIS_chlor.csv.dir <- "/home/amieroh/Documents/Data/Datasets"

region <- "BC" # Benguela Current
# coords <- bbox[, region] # RWS: Where does bbox come from?
coords <- c(-35, -20, 10, 20) # this is the BC

ncList <- list.files(path = MODIS_chlor.dir, pattern = "*.nc", full.names = TRUE, include.dirs = TRUE)
strt.date <- str_sub(basename(ncList[1]), start = 2, end = 8)
end.date <- str_sub(basename(ncList[length(ncList)]), start = 2, end = 8)
nc.init <- nc_open(ncList[1])
LatIdx <- which(nc.init$dim$lat$vals > coords[1] & nc.init$dim$lat$vals < coords[2])
LonIdx <- which(nc.init$dim$lon$vals > coords[3] & nc.init$dim$lon$vals < coords[4])
nc_close(nc.init)


ncFun <- function(nc.file = nc.files, csv.dir = csv.dir) {
nc <- nc_open(nc.file)
  instrument <- ncatt_get(nc, 0, "instrument")$value
  platform <- ncatt_get(nc, 0, "platform")$value
  product_name <- ncatt_get(nc, 0, "product_name")$value
  fNameStem <- substr(product_name, 17, 38)
  timeStamp <- substr(product_name, 2, 8)
  origin <- paste0(substr(timeStamp, 1, 4), "-01-01")
  date <- as.Date(as.numeric(substr(timeStamp, 5, 7)), origin)
  chl <- round(ncvar_get(nc,
                   varid = "chlor_a",
                   start = c(LonIdx[1], LatIdx[1]),
                   count = c(length(LonIdx), length(LatIdx))),
               3)
  dimnames(chl) <- list(lon = nc$dim$lon$vals[LonIdx],
                        lat =  nc$dim$lat$vals[LatIdx])
  nc_close(nc)
  chl <-
    as.data.table(melt(chl, value.name = "chl"), row.names = NULL) %>%
    mutate(t = date) %>%
    na.omit()
  fwrite(chl,
         file = paste(csv.dir, "/", region, "-", instrument, ".",platform, ".",
                      fNameStem, "-", strt.date, "-", end.date, ".csv", sep = ""),
         append = TRUE, col.names = FALSE)
  rm(chl)
}

llply(ncList, ncFun, csv.dir = MODIS_chlor.csv.dir, .parallel = TRUE)

# MODIS_Chloro <- "~/Documents/Masters_2019/MastersProject/Data"
# MODIS_Chloro <- fread(paste0(MODIS_Chloro, "/BC-MODIS.Aqua.L3m_8D_CHL_chlor_a_9km-2002185-2018345.csv"),
#             col.names = c("lon", "lat", "chloro", "date"))

# save(MODIS_Chloro, file = "Data/MODIS_Chloro.RData")
```

## Setting up the chlorophylla dataset

```{r}

# Loading the data
load("Data_P1/MODIS_Chloro.RData") # Extracted/Raw chlorophyll data
load("Data_P1/site_list_sub.Rdata") # Site list of all the sites along the west coast

# Visualisation
chloro_plot <- MODIS_Chloro %>%
  filter(date == min(date)) %>%
  ggplot(aes(x = lon, y = lat)) +
  geom_raster(aes(fill = chloro))

unique_pixel <- MODIS_Chloro %>% 
  select(lon, lat) %>% 
  unique()

match_index <- knnx.index(data = as.matrix(unique_pixel[,1:2]),
                          query = as.matrix(site_list_sub[,5:6]), k = 1)

# Matching the chlorophylla-a lats and lons to the study sites
pixel_match <- unique_pixel[match_index,] %>%
  unite(col = combi, lon, lat, sep = "/", remove = F) %>%
  mutate(site = site_list_sub$site)

chloro_match <- MODIS_Chloro %>%
  unite(col = combi, lon, lat, sep = "/", remove = F) %>%
  filter(combi %in% pixel_match$combi)

chloro_match %>%
  filter(date == max(date)) %>%
  ggplot(aes(x = lon, y = lat)) +
  geom_raster(aes(fill = chloro))

chloro_sites <- chloro_match %>%
  left_join(pixel_match, by = c("combi", "lon", "lat")) %>% 
  dplyr::mutate(date = as.Date(date))
# save(chloro_sites, file = "Data_P1/chloro_sites.RData")

# Visualisation
Chloro_plot_complete <- ggplot(chloro_sites, aes(x = date, y = chloro)) +
  geom_line() +
  facet_wrap(~site, ncol = 1)
Chloro_plot_complete

# Study period from 2010 - 2014
chloro_data <- chloro_sites %>% 
  filter(year(date) %in% seq(2010, 2014))
# save(chloro_data, file = "Data_P1/chloro_data.RData")

# Plotting only years 2014-2015
Chloro_plot_filtered <- ggplot(chloro_data, aes(x = date, y = chloro)) +
  geom_line() +
  facet_wrap(~site, ncol = 1)
Chloro_plot_filtered

# Creating daily temperatures
load("Data_P1/chloro_data.RData") # Loading in the chlorophyll-a data

daily_chloro_data <- chloro_data %>%
  # dplyr::mutate(date = as.Date(date)) %>% # RWS: The values already are dates
  dplyr::group_by(combi, lon, lat, date) %>%
  dplyr::summarise(chloro = mean(chloro, na.rm = TRUE)) %>%
  dplyr::group_by(combi, lon, lat) %>% # RWS: Deleted the date grouping variable being used here
  tidyr::nest() %>%
  dplyr::mutate(clims = purrr::map(data, ts2clm, x = date, y = chloro,
                     climatologyPeriod = c("2010-02-11", "2014-12-20"),
                     maxPadLength = 8)) %>% # Adjust maxPadLength = X for longer interpolation
  dplyr::select(-data) %>%
  tidyr::unnest() %>%
  select(-doy, -seas, -thresh)
```

## EOF's

```{r}

```

In particular, trends will be observed to be different at ocean and coastal locations and we hypothesize that
those differences are due to changes in upwelling intensity.

- Detemining the upwelling indeces in R
- How does upwelling indeces vary at the different distances along the coast
- Positive indeces display a positive upwelling. 


Things to do

- Plot seasonal detrend
- chlorophyll a data
- fix windrose plot
- Stastical analyses